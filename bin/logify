#!/usr/bin/env python3

"""
logify - Simple parser for Couchbase Operator 2.0.0 JSON logs

Parses and renders JSON logs in a human readable format.
"""

import argparse
import json
import os
import os.path
import stat
import tarfile
import tempfile
import time
import webbrowser
import yaml

import jinja2

# pylint: disable=line-too-long

# These are the keys that are required/expected by each log entry.
REQUIRED_KEYS = [
    u'level',
    u'ts',
    u'msg',
]

def init(args, tempdir):
    """
    Init processes any arguments and returns any contextial
    information about the logs.
    """

    # Default to old logs, where you refefence only the file (pre 2.2)
    logs = args.logfile
    metadata = None

    statistics = os.stat(args.logfile)

    # Is it an archive?  Decompress for the duration of this function,
    # and replace the filename with the extracted archive.
    if stat.S_ISREG(statistics.st_mode) and tarfile.is_tarfile(args.logfile):
        archive = tarfile.open(args.logfile)
        archive.extractall(tempdir)
        archive.close()

        # Get the archive path, bit of a hack, but we only assume the archive
        # contains a toplevel cbopinfo-* directory.
        with os.scandir(tempdir) as entries:
            for entry in entries:
                if entry.name.startswith('.'):
                    continue

                args.logfile = os.path.join(tempdir, entry.name)
                break

    # Normalize file paths.
    args.logfile = os.path.abspath(args.logfile)

    # Get the metadata, extract the operator logs path.  Also load up
    # any resources and add them to the metadata e.g. events and resources.
    statistics = os.stat(args.logfile)
    if stat.S_ISDIR(statistics.st_mode):
        with open(args.logfile + '/metadata.json') as filedesc:
            metadata = json.loads(filedesc.read())

        # No logs defined... really bad fail.
        if 'operator' not in metadata:
            logs = None
        else:
            operator_logs = metadata['operator']['logPath']
            logs = os.path.join(os.path.split(args.logfile)[0], operator_logs)

        for cluster in metadata['clusters']:
            cluster_yaml = cluster['resourcePath']
            with open(os.path.join(os.path.split(args.logfile)[0], cluster_yaml)) as filedesc:
                cluster['resource'] = filedesc.read()

            cluster_spec = yaml.load(cluster['resource'], Loader=yaml.SafeLoader)
            if 'status' in cluster_spec and 'conditions' in cluster_spec['status']:
                cluster['conditions'] = cluster_spec['status']['conditions']

            if 'eventsPath' in cluster:
                events_yaml = cluster['eventsPath']
                with open(os.path.join(os.path.split(args.logfile)[0], events_yaml)) as filedesc:
                    events = yaml.load(filedesc.read(), Loader=yaml.SafeLoader)
                    events = sorted(events, key=lambda event: event['metadata']['creationTimestamp'])
                    cluster['events'] = events

    return logs, metadata


def process_logs(logs):
    """
    Take the raw logs and remove any non JSON, normalize fields and perform
    any necessary post-processing.
    """

    # It's possible for a collection to have no logs in at all, because the user
    # hasn't bothered to deploy the operator.
    if not logs:
        return []

    # Read in the file, discarding whitespace and splitting into lines.
    with open(logs) as filedesc:
        log = filedesc.read().strip().split('\n')

    # Parse each line into raw JSON
# Field/customers are stupid and hand us garbled nonsense all the time...
#    log2 = []
#    for x in log:
#        if x.startswith('{'):
#            print(x)
#            log2.append(json.loads(x))

    log = [json.loads(x) for x in log if x.startswith('{')]

    def convert(entry):
        """
        Takes a raw object and converts it into our internal representation.
        """

        # Extract custom labels, doing any conversion on the way.
        labels = {k: entry[k] for k in entry if k not in REQUIRED_KEYS}

        # Convert from a cluster object to a string so it's easier to handle in angular.
        if 'cluster' in labels and isinstance(labels['cluster'], dict):
            labels['cluster'] = labels['cluster']['namespace'] + '/' + labels['cluster']['name']

        # Bump up the level of entries with errors in them.
        if 'error' in labels:
            entry['level'] = 'error'

        return {
            u'level': entry['level'],
            u'timestamp': time.ctime(entry['ts']),
            u'message': entry['msg'],
            u'labels': labels,
        }

    return [convert(x) for x in log]


def main():
    """
    Main entry point
    """

    parser = argparse.ArgumentParser()
    parser.add_argument('logfile')
    args = parser.parse_args()

    # This is the page rendering context, accumulate information as we go along.
    context = {}

    # We may need scratch space for processing, keep it open for the lifetime
    # of the script.
    tempdir = tempfile.TemporaryDirectory()

    # Parse the arguments and get any common configuration.
    logs, metadata = init(args, tempdir.name)

    if metadata:
        context['metadata'] = metadata

    # Load an process the main operator logs.
    log = process_logs(logs)

    context['entries'] = json.dumps(log)

    # Load up the resources to be rendered into a static HTML page.
    base = os.path.dirname(os.path.realpath(__file__))
    with open(base + '/../css/default.css') as tmp:
        css = tmp.read()
        context['css'] = css

    with open(base + '/../templates/index.html') as tmp:
        template = jinja2.Template(tmp.read())

    # Create a temporary file and render the content out to it.
    filedesc, path = tempfile.mkstemp(suffix='.html')
    os.write(filedesc, template.render(**context).encode('utf-8'))
    os.close(filedesc)

    # Finally... magic!
    webbrowser.open_new_tab('file://' + path)


if __name__ == '__main__':
    main()

# vi: ts=4 et:
